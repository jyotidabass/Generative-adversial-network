{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative adversial network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9jZAgb4Q6GrkU/ME+1Pr3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Generative-adversial-network/blob/main/Generative_adversial_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import metrics\n",
        "import scipy.misc\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "\n",
        "\n",
        "def get_optimizer():\n",
        "    return Adam(lr=1e-4)\n",
        "\n",
        "def generator_model(pretrained_weights = None,input_size = (256,256,1),biggest_layer = 512):\n",
        "    \n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(biggest_layer//2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(biggest_layer//2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(biggest_layer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(biggest_layer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "\n",
        "    merge6 = concatenate ([drop4,up6])\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate ([conv3,up7])\n",
        "\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate ([conv2,up8])\n",
        "\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "\n",
        "    merge9 = concatenate ([conv1,up9])  \n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def discriminator_model(input_size = (256,256,1)):\n",
        "\n",
        "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
        "\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if bn:\n",
        "            d = BatchNormalization(momentum=0.8)(d)\n",
        "        return d\n",
        "\n",
        "    img_A = Input(input_size)\n",
        "    img_B = Input(input_size)\n",
        "\n",
        "    df=64\n",
        "\n",
        "    combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
        "\n",
        "    d1 = d_layer(combined_imgs, df, bn=False)\n",
        "    d2 = d_layer(d1, df*2)\n",
        "    d3 = d_layer(d2, df*4)\n",
        "    d4 = d_layer(d3, df*4)\n",
        "    \n",
        "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same', activation='sigmoid')(d4)\n",
        "\n",
        "    discriminator = Model([img_A, img_B], validity)\n",
        "    discriminator.compile(loss='mse', optimizer=get_optimizer(), metrics = ['accuracy'])\n",
        "    \n",
        "    return discriminator\n",
        "\n",
        "\n",
        "\n",
        "def get_gan_network(discriminator, generator, input_size = (256,256,1)):\n",
        "    discriminator.trainable = False\n",
        "    \n",
        "    gan_input2 = Input(input_size)\n",
        "    \n",
        "    x = generator(gan_input2)\n",
        "    valid = discriminator([x,gan_input2])\n",
        "    gan = Model(inputs=[gan_input2], outputs=[valid,x])\n",
        "    gan.compile(loss=['mse','binary_crossentropy'],loss_weights=[1, 100], optimizer=get_optimizer(),metrics = ['accuracy'])\n",
        "    return gan"
      ],
      "metadata": {
        "id": "ucyajDcMiucM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def psnr(img1, img2):\n",
        "    mse = np.mean( (img1 - img2) ** 2 )\n",
        "    if (mse == 0):\n",
        "        return (100)\n",
        "    PIXEL_MAX = 1.0\n",
        "    return (20 * math.log10(PIXEL_MAX / math.sqrt(mse)))\n",
        "\n",
        "def split2(dataset,size,h,w):\n",
        "    newdataset=[]\n",
        "    nsize1=256\n",
        "    nsize2=256\n",
        "    for i in range (size):\n",
        "        im=dataset[i]\n",
        "        for ii in range(0,h,nsize1): #2048\n",
        "            for iii in range(0,w,nsize2): #1536\n",
        "                newdataset.append(im[ii:ii+nsize1,iii:iii+nsize2,:])\n",
        "    \n",
        "    return np.array(newdataset) \n",
        "def merge_image2(splitted_images, h,w):\n",
        "    image=np.zeros(((h,w,1)))\n",
        "    nsize1=256\n",
        "    nsize2=256\n",
        "    ind =0\n",
        "    for ii in range(0,h,nsize1):\n",
        "        for iii in range(0,w,nsize2):\n",
        "            image[ii:ii+nsize1,iii:iii+nsize2,:]=splitted_images[ind]\n",
        "            ind=ind+1\n",
        "    return np.array(image)  \n",
        "\n",
        "\n",
        "\n",
        "def getPatches(watermarked_image,clean_image,mystride):\n",
        "    watermarked_patches=[]\n",
        "    clean_patches=[]\n",
        "    \n",
        "    \n",
        "    h =  ((watermarked_image.shape [0] // 256) +1)*256 \n",
        "    w =  ((watermarked_image.shape [1] // 256 ) +1)*256\n",
        "    image_padding=np.ones((h,w))\n",
        "    image_padding[:watermarked_image.shape[0],:watermarked_image.shape[1]]=watermarked_image\n",
        "    \n",
        "    for j in range (0,h-256,mystride):  #128 not 64\n",
        "        for k in range (0,w-256,mystride):\n",
        "            watermarked_patches.append(image_padding[j:j+256,k:k+256])\n",
        "    \n",
        "    \n",
        "    h =  ((clean_image.shape [0] // 256) +1)*256 \n",
        "    w =  ((clean_image.shape [1] // 256 ) +1)*256\n",
        "    image_padding=np.ones((h,w))*255\n",
        "    image_padding[:clean_image.shape[0],:clean_image.shape[1]]=clean_image\n",
        "\n",
        "    for j in range (0,h-256,mystride):    #128 not 64\n",
        "        for k in range (0,w-256,mystride):\n",
        "            clean_patches.append(image_padding[j:j+256,k:k+256]/255)  \n",
        "            \n",
        "    return np.array(watermarked_patches),np.array(clean_patches)"
      ],
      "metadata": {
        "id": "1wogX3fYjB2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}